{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Sparsel21Autoencoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-69e0037f8f36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mSparsel21Autoencoder\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msdae\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mshrink\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0ml21shrink\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mSHR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Sparsel21Autoencoder'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import Sparsel21Autoencoder as sdae\n",
    "from shrink import l21shrink as SHR \n",
    "\n",
    "class RobustSparseAutoencder():\n",
    "    \"\"\"\n",
    "    @author: Chong Zhou\n",
    "    \n",
    "    Updated to python3\n",
    "    \n",
    "    Des:\n",
    "        X = L + S\n",
    "        L is a non-linearly low dimension matrix and S is a sparse matrix.\n",
    "        argmin ||L - Decoder(Encoder(L))||+ ||Encoder(L)||_2,1 + ||S||_2,1\n",
    "        Use Alternating projection to train model\n",
    "        The idea of shrink the l21 norm comes from the wiki 'Regularization' link: {\n",
    "            https://en.wikipedia.org/wiki/Regularization_(mathematics)\n",
    "        }\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, sess, layers_sizes, sparsities, lambda_=1.0):\n",
    "        self.lambda_ = lambda_\n",
    "        self.layers_sizes = layers_sizes\n",
    "        self.sparcities = sparsities\n",
    "        self.errors=[]\n",
    "        assert len(sparsities) == len(layers_sizes) - 1\n",
    "\n",
    "        self.SAE = sdae.Sparsel21_Deep_Autoencoder( sess = sess, input_dim_list = self.layers_sizes,\n",
    "                                                    sparsities = self.sparcities)\n",
    "\n",
    "\n",
    "    def fit(self, X, sess, learning_rate=0.05, inner_iteration = 50,\n",
    "            iteration=20, batch_size=40, verbose=False):\n",
    "        ## The first layer must be the input layer, so they should have same sizes.\n",
    "        assert X.shape[1] == self.layers_sizes[0]\n",
    "        ## initialize L, S, mu(shrinkage operator)\n",
    "        self.L = np.zeros(X.shape)\n",
    "        self.S = np.zeros(X.shape)\n",
    "        #LS0 = self.L + self.S\n",
    "        ## To estimate the size of input X\n",
    "\n",
    "        if verbose:\n",
    "            print (\"X shape: \", X.shape)\n",
    "            print( \"L shape: \", self.L.shape)\n",
    "            print (\"S shape: \", self.S.shape)\n",
    "            print (\"X sum value\", np.linalg.norm(X,'fro'))\n",
    "\n",
    "        for it in range(iteration):\n",
    "            if verbose:\n",
    "                print (\"Out iteration: \" , it)\n",
    "            ## alternating project, first project to L\n",
    "            self.L = np.array(X - self.S,dtype=float)\n",
    "            ## Using L to train the auto-encoder\n",
    "            self.SAE.fit(self.L, sess = sess,\n",
    "                                    iteration = inner_iteration,\n",
    "                                    learning_rate = learning_rate,\n",
    "                                    batch_size = batch_size,\n",
    "                                    verbose = verbose)\n",
    "            ## get optmized L\n",
    "            self.L = self.SAE.getRecon(X = self.L, sess = sess)\n",
    "            ## alternating project, now project to S and shrink S\n",
    "            self.S = self.l21shrink(self.lambda_, (X - self.L).T).T\n",
    "\n",
    "        return self.L , self.S\n",
    "\n",
    "    def transform(self, X, sess):\n",
    "        return self.SAE.transform(X = X, sess = sess)\n",
    "    def getRecon(self, X, sess):\n",
    "        return self.SAE.getRecon(X, sess = sess)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x = np.load(r\"../data/data.npk\")\n",
    "    # x = np.array(x,dtype=float)\n",
    "    with tf.Session() as sess:\n",
    "        rsae = RobustSparseAutoencder(sess = sess, lambda_= 4000, layers_sizes=[784,784,784,784],sparsities=[0.5,0.5,0.5])\n",
    "\n",
    "        L, S = rsae.fit(x, sess = sess, inner_iteration = 20, iteration = 30,verbose = True)\n",
    "        print (L.shape,S.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
